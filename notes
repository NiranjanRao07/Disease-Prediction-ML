->implement a robust machine learning model that can 
efficiently predict the disease of a human, based on the symptoms that he/she posses.

-> dataset from Kaggle for this problem. This dataset consists of two CSV files one for training 
and one for testing. There is a total of 133 columns in the dataset out of which 132 columns 
represent the symptoms and the last column is the prognosis.

-> all the columns are numerical, the target column i.e. prognosis is a string type and 
is encoded to numerical form using a label encoder. dataset is a clean dataset with no 
null values and all the features consist of 0’s and 1’s.

->K-Fold cross-validation to evaluate the machine learning models, Support Vector Classifier, Naive Bayes Classifier, and Random Forest Classifier for cross validation.

->predicting the disease for the input symptoms by combining the predictions of all
three models. This makes overall prediction more robust and accurate.

-> splitting the data into 80:20 format i.e. 80% of the dataset will be used for 
training the model and 20% of the data will be used to evaluate the performance of the models.

->why we are using the respective models:

=>K-Fold Cross-Validation: K-Fold cross-validation is one of the cross-validation techniques in which the whole dataset is split into k number of subsets, also known as folds, then training of the model is performed on the k-1 subsets and the remaining one subset is used to evaluate the model performance.

=>Support Vector Classifier: Support Vector Classifier is a discriminative classifier i.e. when given a labeled training data, the algorithm tries to find an optimal hyperplane that accurately separates the samples into different categories in hyperspace.

=>Gaussian Naive Bayes Classifier: It is a probabilistic machine learning algorithm that internally uses Bayes Theorem to classify the data points.

=>Random Forest Classifier: Random Forest is an ensemble learning-based supervised machine learning classification algorithm that internally uses multiple decision trees to make the classification. In a random forest classifier, all the internal decision trees are weak learners, the outputs of these weak decision trees are combined i.e. mode of all the predictions is as the final prediction. 

=>A correlation matrix is simply a table which displays the correlation coefficients for different variables. The matrix depicts the correlation between all the possible pairs of values in a table. It is a powerful tool to summarize a large dataset and to identify and visualize patterns in the given data.

LITERATURE SURVEY

1) XGBoost, Linear Discriminant Analysis (LDA), and Naive Bayes (NB). 
   Patients needn't have to go to the clinic and can self diagnose using this ML model, reduces time.
   Since some of the values are input-based, accuracy decreases due to human error.

2) Suport vector machine, Naive Bayes (NB). , Random forest model 
   Machine Learning is responsible for cutting the workload and time since, the algorithm does the hard work. Since, ML is reliable and enchances creativity.
   The missing and null values dataset is not used. This is adds ambiguity in to the ML model.

3) Suport vector machine, Naive Bayes (NB), KNN
   It can minimize computational complexity while simulating the expert radiologist’s reasoning and style. If the user inputs parameters such as contour, form, and density, the algorithm offers a cancer categorization based on their preferred method 
   Disadvantage of this work is that it used a simplified Diabetes dataset with only eight binary-classified parameters. As a result, getting 100% accuracy with a less difficult dataset is unsurprising. Furthermore, there is no discussion of how the algorithms influence the final prediction or how the result should be viewed from a nontechnical position in the experiment. 



 
 